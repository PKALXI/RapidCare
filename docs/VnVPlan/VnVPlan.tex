\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\usepackage[round]{natbib}

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
04-11-2024 & 1.0 & Initial Version\\
10-03-2025 & 1.1 & Updates in Accordance to VnV report\\
31-03-2025 & 1.1 & Updated NFR and Security requirement tests to reflect changes in SRS and Hazard Analysis\\
31-03-2025 & 1.1 & Updates to reflect changes in VnV Report\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms} \label{section:1}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  SRS & Software Requirements Specification\\
  MG & Module Guide\\
  MIS & Module Interface Specification\\
  FR & Functional Requirements\\
  NFR & Non-Functional Requirements\\
  UI & User Interface\\
  PIPEDA & Personal Information Protection and Electronic Documents Act\\
  EHR & Electronic Healthcare Record\\
  API & Application Programming Interface \\
  IR & Integrity Requirement \\
  AC & Access Requirement \\
  \bottomrule
\end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

\newpage

\pagenumbering{arabic}

\section{General Information} \label{section:2}

\subsection{Summary} \label{section:2.1}

This document provides a comprehensive description of the system tests specific to all requirements for a software application, RapidCare, that aims to streamline the healthcare documentation process aimed to be run as a web application. This document will be used as a verification tool to ensure that system fulfills all the requirements and needs of the user.
This document will allow for an in-depth description of system tests for functional, non-functional, safety and security requirements. Additionally, it will outline the verification plan for design and implementation which will make sure that all requirements are fulfilled.\\
Along with this, the document will also include the software validation plan that will validate the requirements by making sure that the client who intends to use this application is satisfied with its features.\\
The system will allow the users to automate the documentation process by transcribing audio to text and generating reports based on the transcribed data. The system will also provide diagnostics and treatment plan suggestions based on the reports to further optimize the process.    

\subsection{Objectives} \label{section:2.2}

The objective of verification and validation plan is to build confidence in the correctness of the software and decide if the software is correctly following the requirements. Additionally, we want to achieve adequate usability of our software by making sure it satisfies with the user's needs, thus speeding up the documentation process. Another goal of this document is to test the accuracy of the input data to reflect that the software suggests correct diagnosis based on the text that has been transcribed from the audio conversation. Since the safety and security of the patient's information is a priority for this software, the plan is to conduct tests to make sure patient's information remains safe and secure.\\
The main focus will be on verifying the system's essential features to make sure it complies with safety regulations and protects patient information and healthcare professionals' confidence. Assuming that the external libraries are already tested by their implementation team, the priority of this document will be to test the requirements of the software to ease the patient care.\\
Due to time constraints and limited resources, we are not going to test the quality of usability. While the usability is important to test, the priority of this document is to test the fundamental requirements to ensure the software is accurate and reliable. 

\subsection{Challenge Level and Extras} \label{section:2.3}

In terms of the project challenge level, the project will come in the general category. The reasoning for this choice is supported below:

\begin{itemize}
  \item\textbf{Domain Knowledge} -- The documentation process has a lot of ins and outs which may differ between health organizations. Our supervisors and stakeholders will provide us with a base of the requirements, but further elicitation will be required to ensure that the requirements reflect a problem that truly exists. Additionally, since the whole patient journey is tracked we will have to survey other hospital staff as well to gain a further understanding. 
  \item\textbf{Implementation Challenges} -- There will be quite a few microservices required for this project where each microservice has high complexity. The performance of the microservices must be high as this use case requires quick response time. Additionally, since we are dealing with patient data security and privacy must be upheld. Lastly, the integration between all of the parts must be secure and undergo rigorous integration testing.
\end{itemize}

As part of the extras for this project, we will accomplish the following extras:

\begin{itemize}
  \item \textbf{Usability testing} -- This is designed for the users to assess how easily they can navigate and use the software. This will ensure that the requirements of the software perfectly align with the user's needs.
  \item \textbf{User Manual} -- This will include instructions that will help the user to get started with the software. It will have an overview of the software along with some help resources for setup instructions and easy navigation for the user.
\end{itemize}

\subsection{Relevant Documentation} \label{section:2.4}

The documents that are relevant to this project are:
\begin{itemize}
  \item \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS} 
  \item \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/HazardAnalysis/HazardAnalysis.pdf}{Hazard analysis}
  \item \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}
  \item \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}
\end{itemize}
SRS and Hazard analysis documents list the FR, NFR, safety and security requirements which will assist us in developing tests for each of them. Moreover, MG and MIS documents give a structured approach of system's architecture and interface, ensuring that the system is thoroughly tested. This will help us to develop test cases for vital areas of the software.  


\section{Plan} \label{section:3}

% \wss{Introduce this section.  You can provide a roadmap of the sections to come.}
This section will include details about the verification and validation team. In addition to this, this section will include SRS, design, validation and verification plan, and implementation verification plan. Lastly, it will outline details about automated testing and verification tools and software validation plan.

\subsection{Verification and Validation Team} \label{section:3.1}

% \wss{Your teammates.  Maybe your supervisor.
%   You should do more than list names.  You should say what each person's role is
%   for the project's verification.  A table is a good way to summarize this information.}
Here is a basic outline of the verification and validation team and their responsibilities. Please note that team members will switch roles between the tasks to ensure a well-rounded verification of the system. All team members will work collectively toward user documentation for this project.\\
\textbf{Gurleen Rahi}: Will focus on functional testing, specifically for transcription and report generation module. Will also focus on usability survey and design verification throughout the project.\\
\textbf{Pranav Kalsi}: Will focus on functional testing, specifically functional tests related to machine learning models. Also, will focus on integration testing and code verification throughout the project.\\
\textbf{Inreet Kaur}: Will focus on functional testing, specifically for various components in the data layer. Will also focus on safety requirement testing for this project.\\
\textbf{Moamen Ahmed}: Will focus on functional testing specifically for authentication and account management components. Will also focus on non-functional testing of the system.\\
\textbf{Kristen Burrows} (Supervisor): Will help the team verify and provide feedback to improve the SRS, design, and verification and validation plan verification plan.\\


\subsection{SRS Verification Plan} \label{section:3.2}

% \wss{List any approaches you intend to use for SRS verification.  This may
%   include ad hoc feedback from reviewers, like your classmates (like your
%   primary reviewer), or you may plan for something more rigorous/systematic.}
% \wss{If you have a supervisor for the project, you shouldn't just say they will
% read over the SRS.  You should explain your structured approach to the review.
% Will you have a meeting?  What will you present?  What questions will you ask?
% Will you give them instructions for a task-based inspection?  Will you use your
% issue tracker?}
% \wss{Maybe create an SRS checklist?}
Each team member will perform a detailed review of the SRS to verify clarity, feasibility, and consistency across requirements. Identified issues will be added to the appropriate GitHub issue of the specific section. The team members can then make appropriate changes based on the feedback. Each team member will complete functional requirement tests for their assigned components as well as non-functional and safety requirement testing (as assigned). Once completed, at least two other team members will review and verify the tests using the sample input for the tests and verify the desired behavior. We will also ask for feedback from our peers, i.e. have another team review SRS and update it based on the feedback in the GitHub issues.\\ 
In addition to this, we will then conduct a structured meeting review with our supervisor to gain feedback on the correctness and relevance of the requirements of the SRS document. During this meeting, we will have a walk-through of the key sections of the SRS, outline any challenging or critical requirements, and discuss any issues flagged during internal reviews.\\
Once the system is complete, we will then have the supervisor and other stakeholders verify the system's functionality using specified inputs. They will also complete the usability survey (section \ref{section:6.2}) for the system.


\subsection{Design Verification Plan} \label{section:3.3}

% \wss{Plans for design verification}
% \wss{The review will include reviews by your classmates}
% \wss{Create a checklists?}
Our design verification plan ensures that the system design meets requirements for consistency, maintainability, scalability, and usability. This process will involve systematic reviews, integration testing, and structured feedback sessions with the supervisor and stakeholders.\\
Each team member will review the design of specific components to verify consistency with project requirements, maintainability, and scalability. Identified issues will be added to the appropriate GitHub issue of the specific section. The team members can then make appropriate changes based on the feedback. The team will also conduct integration testing to check the compatibility of different components of the system and compatibility with the hardware.\\
Once the system is completed, the team will conduct a review session with the supervisor. In this meeting, we will provide a high-level overview of architecture and key components. We will present a sample user journey and gather feedback on the system's design for improvement.\\

Following is a checklist for the design verification plan: 
\begin{itemize}
  \item Verify the design meets all requirements identified in SRS and hazard analysis
  \item Verify each module is modular and maintainable
  \item Verify the design meets the coding standards
  \item Verify hardware and software compatibility
  \item Verify alignment with project objectives and stakeholder needs
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan} \label{section:3.4}

% \wss{The verification and validation plan is an artifact that should also be
% verified. Techniques for this include review and mutation testing.}
% \wss{The review will include reviews by your classmates}
% \wss{Create a checklists?}
This document must be verified to make sure it is robust and reliable. The plan must have extensive coverage to ensure that all requirements are covered. The two main methods we will be using are the following:

\begin{itemize}
  \item \textbf{VnV Reviews}: We will identify gaps in the Validation and Verification plan through reviews that we conduct with our peers and supervisor. This will help ensure completeness, accuracy, and feasibility of the plan.
    \begin{itemize}
      \item \textbf{Checklist for Review:}
      \begin{itemize}
        \item Verify that all requirements are covered in the plan
        \item Ensure that the plan adheres to industry standards
        \item Check for consistency and clarity in the plan
        \item Ensure that the plan is feasible and can be carried out by any reader
        \item Post review send respective items to developers (Pranav, Inreet, Gurleen, Moamen) to be resolved.
      \end{itemize}
    \end{itemize}

\end{itemize}

Through both of these processes we will be able to ensure that the plan covers all input and is clear and consistent such that it can be carried out by any reader.

\subsection{Implementation Verification Plan} \label{section:3.5}

% \wss{You should at least point to the tests listed in this document and the unit
%   testing plan.}
% \wss{In this section you would also give any details of any plans for static
%   verification of the implementation.  Potential techniques include code
%   walkthroughs, code inspection, static analyzers, etc.}
% \wss{The final class presentation in CAS 741 could be used as a code
% walkthrough.  There is also a possibility of using the final presentation (in
% CAS741) for a partial usability survey.}

The implementation verification plan is really a lot of static review this is include code walkthroughs and code inspections in conjunction with static analyzers. The plan will be as follows:

\begin{enumerate}
  \item Static analysis will be present at the CI/CD pipeline level, utilizing tools like \href{https://github.com/super-linter/super-linter}{Super-Linter} in our GitHub Actions sequence, to catch code errors or style violations whenever code is committed or a pull request is created, this will also expedite the code review process as we can have a level of checking to catch errors.
  \item Next, the team will conduct a high-level code walkthrough to identify any high-level logic errors. This will also verify the service interaction flows to ensure the desired business logic is fulfilled.
  \item Finally, in any critical sections or features, we will perform a thorough code inspection, using the following checklist to ensure the implementation is verified:
    \begin{itemize}
      \item Functionality aligns with requirements
      \item Error handling is implemented correctly
      \item Code is modular, maintainable and extendable
    \end{itemize}

  Finally, the way critical sections will be identified is through the requirements, as well as reviews with our supervisor to ensure the critical use case flows are highlighted and verified.
\end{enumerate}

This strategy will be implemented in section \ref{section:4}.

\subsection{Automated Testing and Verification Tools} \label{section:3.6}

% \wss{What tools are you using for automated testing.  Likely a unit testing
%   framework and maybe a profiling tool, like ValGrind.  Other possible tools
%   include a static analyzer, make, continuous integration tools, test coverage
%   tools, etc.  Explain your plans for summarizing code coverage metrics.
%   Linters are another important class of tools.  For the programming language
%   you select, you should look at the available linters.  There may also be tools
%   that verify that coding standards have been respected, like flake9 for
%   Python.}
% \wss{If you have already done this in the development plan, you can point to
% that document.}
% \wss{The details of this section will likely evolve as you get closer to the
%   implementation.}
An outline of the CI/CD strategy can be found in section 7 of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/DevelopmentPlan/DevelopmentPlan.pdf}{Development Plan}. A list of relevant frameworks will be found in section 10 of the same document.

\subsection{Software Validation Plan} \label{section:3.7}

% \wss{If there is any external data that can be used for validation, you should
%   point to it here.  If there are no plans for validation, you should state that
%   here.}
% \wss{You might want to use review sessions with the stakeholder to check that
% the requirements document captures the right requirements.  Maybe task based
% inspection?}
% \wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
% be used as an opportunity to validate the requirements.  You should plan on 
% demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
% The feedback from your supervisor will be very useful for improving your project.}
% \wss{For teams without an external supervisor, user testing can serve the same purpose 
% as a Rev 0 demo for the supervisor.}
% \wss{This section might reference back to the SRS verification section.}

Validation of the project is critical and our supervisor will be vital for this process. Our supervisor will make sure that our software solves the user groups needs. Our supervisor is in an excellent position to validate the software as not only are they a domain expert they also fit the potential user group.\\

In the review sessions we intend on hold with our we ensure to validate the following to ensure the right software is built:

\begin{itemize}
  \item Validate that the requirements cover all user needs and expectation (walkthrough of complete SRS).
  \item Confirm the feasibility of the requirements such that we can validate if they can even be built in the real-world.
  \item Does the software align with the expectations of stakeholders.
  \item Highlight gaps in compliance and fulfillment of industry standards.
  \item Post review send respective items to developers (Pranav, Inreet, Gurleen, Moamen) to be resolved.
\end{itemize}

Completing this checklist will ensure the software is validated.

\section{System Tests} \label{section:4}

% \wss{There should be text between all headings, even if it is just a roadmap of
% the contents of the subsections.}
This section will include system tests for functional, non-functional, and safety and security requirements. In addition to this, we will include a traceability table for test cases and requirements.

\subsection{Tests for Functional Requirements} \label{section:4.1}

% \wss{Subsets of the tests may be in related, so this section is divided into
%   different areas.  If there are no identifiable subsets for the tests, this
%   level of document structure can be removed.}

% \wss{Include a blurb here to explain why the subsections below
%   cover the requirements.  References to the SRS would be good here.}

This section contains the tests for the Functional Requirements. The subsections for these tests were created based on the subsections of the Functional Requirements listed in the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS}. Traceability for these requirements and tests can be found in the section \ref{section:4.4}.


\subsubsection{Add a document to the database} \label{section:4.1.1}

This subsection covers FR1, FR4, and FR8 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS document} by testing that the system is able to add a document to the database only when a valid input is provided.

\begin{enumerate}

\item{test-FR1,4,8-1} \label{test-FR1,4,8-1}

Control: Manual

Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input.

Input: Correct and complete input data for all required fields.

Output: A confirmation message and a new entry is added to the appropriate database.

Test Case Derivation: The system will validate the input data, accept a complete and correct data input, and confirm a successful addition to the database.

How test will be performed: The test controller will input a valid data object and check if the system is able to validate and accept the valid input. The controller will verify that a confirmation message appears and there is a valid new entry in the appropriate database.

					
\item{test-FR1,4,8-2} \label{test-FR1,4,8-2}

Control: Manual
					
Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input.

Input: Invalid input data.

Output: An error message outlining the invalid fields along with possible steps to guide the users to recover from the error state.

Test Case Derivation: The system will validate the input data and prompt an error message outlining why the system is not able to accept the input data. No new document is added to the database.

How test will be performed: The test controller will input an invalid data object and check if the system is able to validate and reject the invalid input. The controller will also verify that an error message appears and there is no new entry in the appropriate database.

\end{enumerate}


\subsubsection{Remove a document from the database} \label{section:4.1.2}

This subsection covers FR2, FR5, and FR9 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS document} by testing that the system is able to remove a document to the database only when a valid input idetifier is provided.

\begin{enumerate}

\item{test-FR2,5,9-1} \label{test-FR2,5,9-1}

Control: Manual

Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input. A document exists in the appropriate database.

Input: A correct identifier for the document to be deleted.

Output: A confirmation message and relevant entry no longer exist in the database.

Test Case Derivation: The system should allow the deletion of an existing document when the correct identifier is provided. 

How test will be performed: The test controller will input a valid identifier and check if the system is able to validate and accept the valid input. The controller will verify that a confirmation message appears and the associated document no longer exists in the database.

					
\item{test-FR2,5,9-2} \label{test-FR2,5,9-2}

Control: Manual

Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input. A document exists in the appropriate database.

Input: An invalid identifier for the document to be deleted.

Output: An error message outlining the invalid input along with possible steps to guide the users to recover from the error state.

Test Case Derivation: The system should be able to validate the provided identifier and prevent the deletion of any existing document. 

How test will be performed: The test controller will input an invalid identifier and check if the system is able to validate and reject the invalid identifier. The controller will verify that an error message appears, and no document is deleted from the database.

\end{enumerate}


\subsubsection{Update document in the database} \label{section:4.1.3}

This subsection covers FR3, FR6, FR10, and FR11 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS document} by testing that the system is able to update a document to the database only when a valid input idetifier is provided.

\begin{enumerate}

\item{test-FR3,6,10,11-1} \label{test-FR3,6,10,11-1}

Control: Manual

Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input.

Input: A correct identifier for the document to be updated.

Output: A confirmation message and relevant entry shows the updated data in the database.

Test Case Derivation: The system should allow to update the existing document when the correct identifier is provided.

How test will be performed: The test controller will input a valid identifier and check if the system is able to validate and accept the valid input. The controller will verify that a confirmation message appears and the current document is updated in the database.


\item{test-FR3,6,10,11-2} \label{test-FR3,6,10,11-2}

Control: Manual

Initial State: The system is set up, open to the relevant section (e.g., Healthcare Network, Healthcare Professional, or Patient Records), and ready to take the user input.

Input: Invalid input data.

Output: An error message outlining the invalid fields along with possible steps to guide the users to recover from the error state.

Test Case Derivation: The system should be able to validate the provided input and prevent any invalid updates to the document.

How test will be performed: The test controller will input an invalid input and check if the system is able to validate and reject the invalid inputs. The controller will verify that an error message appears, and no document is updated in the database. 

\end{enumerate}


\subsubsection{Login for valid/invalid credentials} \label{section:4.1.4}

This subsection covers FR7 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf} {SRS document} by testing that the system is able to allow to access the database only when a valid input credentials is provided.

\begin{enumerate}

\item{test-FR7-1} \label{test-FR7-1}

Control: Manual

Initial State:  The system is set up, and the user is on the login page, ready to enter their credentials.

Input: The correct credentials for login.

Output: A confirmation message and user is logged into the system.

Test Case Derivation: The system should be able to validate the provided credentials and allow to login to the system.

How test will be performed: The test controller will input the valid credentials and check if the system is able to validate and accept the valid input. The controller will verify that a confirmation message appears and the user is able to login the database.


\item{test-FR7-2} \label{test-FR7-2}

Control: Manual

Initial State:  The system is set up, and the user is on the login page, ready to enter their credentials.

Input: Invalid credentials for login.

Output: An error message outlining the invalid fields.

Test Case Derivation: The system should be able to validate the provided credentials and prevent unauthorized access to the database.

How test will be performed: The test controller will input any invalid credentials and check if the system is able to validate and reject the invalid credentials. The controller will verify that an error message appears and unauthorized access will be denied.

\end{enumerate}

\subsubsection{Voice-to-text-transcription check} \label{section:4.1.5}

This subsection covers FR7 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf} {SRS document} by testing that the system is able to transcribe audio data to the text.

\begin{enumerate}

\item{test-FR11-1} \label{test-FR11-1}

Control: Manual

Initial State: The user has successfully logged in and is on the appropriate view to take audio input. 

Input: A valid audio chunk from the conversation between the healthcare professional and the patient.

Output: A confirmation message indicating successful transcription and the screen shows the transcribed text.

Test Case Derivation: The system should be able to validate the voice input by processing it and transcribe to text in real-time.

How test will be performed: The test controller will input a valid audio chunk and check if the system is able to transcribe the valid input. The controller will verify that a confirmation message appears and the voice input is transcribed to text.


\item{test-FR11-2} \label{test-FR11-2}

Control: Manual

Initial State: The user has successfully logged in and is on the appropriate view to take audio input.

Input: An invalid data format for transcription.

Output: An error message saying that the file or data format is not valid.

Test Case Derivation: The system should be able to validate the invalid input data format.

How test will be performed: The test controller will input an invalid data format and check if the system is able to validate and reject the input. The controller will verify that an error message appears.

\end{enumerate}


\subsubsection{Validate output of correct diagonsis and treatment plan} \label{section:4.1.6}

This subsection covers FR12 and FR13 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf} {SRS document} by testing that the system is able to create predictions on the diagnosis and treatment plans with a high accuracy and confidence.

\begin{enumerate}

\item{test-FR12,13-1} \label{test-FR12,13-1}

Control: Automatic

Initial State: Patient's blank chart is present, and a transcription of the Patient-Healthcare Professional conversation is present.

Input: Correct and complete input data where all the symptoms fields of the chart are filled out.

Output: A suggestion on what the diagnosis should be based on the symptoms, then based on the diagnosis provide possible treatment plan.

Test Case Derivation: The system will create a validation set of data, where the model will be evaluated. 

How test will be performed: When the model is deployed it will be automatically evaluated such that it will have to have a classification accuracy greater or equal than 85\%.



\subsubsection{Validate input data for models} \label{section:4.1.7}
This subsection covers FR12 and FR13 from of the \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf} {SRS document} by testing that the system is able to validate the input data in the charts such that it may be inputted into the prediction module.


\item{test-FR12,13-2} \label{test-FR12,13-2}

Control: Automatic

Initial State: The patient's blank chart is present.

Input: Correct and complete input data where the transcription is present.

Output: A suggestion on what the diagnosis should be based on the symptoms, then based on the diagnosis provide possible plan.

Test Case Derivation: The system will test if the chart data is relevant and correct.  

How test will be performed: When the system is deployed the model input will be tested such that it only accepts valid inputs.

\item{test-FR12,13-3} \label{test-FR12,13-3}

Control: Automatic

Initial State: The patient's blank chart is present.

Input: Incorrect and incomplete transcription field as it was not recorded/complete.

Output: An error message relaying the error along with a HTTP status code indicating incomplete request.

Test Case Derivation: The system will test if the chart data is relevant and correct.  

How test will be performed: When the system is deployed the model input will be tested such that it only accepts valid inputs. It will output a error message if the output is wrong.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements} \label{section:4.2}

\subsubsection{Look and Feel Requirement (NFR1)} \label{section:4.2.1}

\begin{enumerate}
    \item{test-AD1} \label{test-AD1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: UI is operational and accessible to users.
    
    Input: Users view the UI under normal operating conditions.
    
    Output: Feedback collected on UI's aesthetic appeal and simplicity.
    
    How this test will be performed: A test group of users will be given the system, and a set of routine UI interactions to perform. They will be asked to complete these interactions using the interface. After they do this, they will be given a Usability survey to verify that the interface meets aesthetic appeal and simplicity requirements.

    \item{test-AD2} \label{test-AD2}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: UI is operational and accessible to users.
    
    Input: Users interact with the UI during routine tasks and provide feedback.
    
    Output: Feedback collected on satisfaction with the UI design.
    
    How this test will be performed: A test group of users will be given the system, and a set of common tasks to perform. They will be asked to complete these tasks using the interface. After they do this, they will be given a Usability survey to verify that at least 80\% of users are satisfied with the design.
\end{enumerate}

\subsubsection{Usability Requirement (NFR2)} \label{section:4.2.2}

\begin{enumerate}
    \item{test-UR1} \label{test-UR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Users navigate and explore the system features independently.
    
    Output: Majority of users can locate core functions without additional guidance. System reflects clear discoverability, affordances, and visibility.
    
    How this test will be performed: A test group of users will be given the system, and a set of core functions to locate. They will be asked to find these functions without guidance. After they do this, they will be given the usability survey to verify that users could locate core functions independently.
\end{enumerate}

\subsubsection{Performance Requirement (NFR3)} \label{section:4.2.3}

\begin{enumerate}
    \item{test-PR1} \label{test-PR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: Transcription interface open and ready.
    
    Input: Real-time voice input provided by the user.
    
    Output: Real-time transcription displayed within an acceptable time frame (should not exceed 2 minutes).
    
    How this test will be performed: A test group of users will be given the system, and a set of voice inputs to transcribe. They will be asked to speak these inputs for real-time transcription. After they do this, the transcription delay will be noted and should not exceed the specified time frame.

\end{enumerate}

\subsubsection{Operational Requirement (NFR4)} \label{section:4.2.4}

\begin{enumerate}
    \item{test-OR1} \label{test-OR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Multiple test users perform basic interactions (e.g., logging in, recording voice input, retrieving stored data).
    
    Output: The system responds within an acceptable time frame, and no critical errors occur.
    
    How test will be performed: A test group of users will be given the system, and a set of common tasks to perform. After they complete these tasks, any abnormal behavior and errors will be noted.
\end{enumerate}

\subsubsection{Maintainability Requirement (NFR5)} \label{section:4.2.5}

\begin{enumerate}
    \item{test-MR1} \label{test-MR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Review and modify system components (e.g., refactoring code, adding new functionality).
    
    Output: System successfully applies updates without impacting stability.
    
    How this test will be performed: A set of software updates will be applied, and it will be verified that system functionality remained stable after updates, ensuring that the system supports regular updates for bug fixes and feature enhancements.
\end{enumerate}

\subsubsection{Security Requirement (NFR6)} \label{section:4.2.6}

\begin{enumerate}
    \item{test-SR1} \label{test-SR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Simulate unauthorized access attempts.
    
    Output: Unauthorized attempts are blocked and access logs capture details.
    
    How this test will be performed: A test group of users will be given the system, and a set of unauthorized access scenarios. They will be asked to attempt these scenarios while monitoring system responses. After they do this, it will be verified that all unauthorized attempts were properly blocked.
\end{enumerate}

\subsubsection{Cultural Requirement (NFR7)} \label{section:4.2.7}

\begin{enumerate}
  \item{test-CR1} \label{test-CR1}
  
  Type: Non-functional, Dynamic, Manual
  
  Initial State: System is deployed and accessible.
  
  Input: User interacts with the system in a culturally diverse context, including input that may contain sensitive language.
  
  Output: System processes the input and responds appropriately, ensuring cultural sensitivity and avoiding any inappropriate language in its output.
  
  How this test will be performed: A test group of users will be given the system and a set of culturally diverse input scenarios. Users will interact with the system using these inputs and it will be verified that the system responds in a culturally appropriate manner.
\end{enumerate}

\subsubsection{Legal Requirement (NFR8)} \label{section:4.2.8}

\begin{enumerate}
    \item{test-LR1} \label{test-LR1}
    
    Type: Non-functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Conduct a compliance audit against PIPEDA and other relevant data protection standards.
    
    Output: System passes all compliance checks and shows relevant disclaimers with no exceptions.
    
    How test will be performed: A certified compliance auditor verifies data handling and data access policies, ensuring that the system displays appropriate medical disclaimers and feedback to the users.
\end{enumerate}

\subsubsection{Scalability Requirement (NFR9)} \label{section:4.2.9}

\begin{enumerate}
    \item{test-S1} \label{test-S1}
    
    Type: Non-functional, Dynamic, Automated
    
    Initial State: System deployed on a test server environment capable of scaling horizontally.

    Input: Simulate an increasing number of concurrent users.

    Output: System maintains consistent performance and response times without degradation.
    
    How test will be performed: Use load testing tools like Apache to simulate concurrent user traffic and monitor system response times, server load, and throughput during the test.
    
\end{enumerate}


\subsection{Tests for Safety and Security Requirements} \label{section:4.3}

\subsubsection{AC1}

\begin{enumerate}

    \item{test-AC1-1} \label{test-AC1-1}
    
    Type: Functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Attempt to access system with invalid credentials.
    
    Output: All unauthorized access attempts are blocked.
    
    How this test will be performed: A test group of users will be given the system. They will be asked to attempt access the system with invalid credentials and it will be verified that access was properly blocked.
\end{enumerate}

\subsubsection{AC2}

\begin{enumerate}
    \item{test-AC2-1}  \label{test-AC2-1}
    
    Type: Functional, Dynamic, Manual
    
    Initial State: System is deployed and accessible.
    
    Input: Attempt to create, update, and delete user accounts using non-admin credentials.
    
    Output: System correctly displays UI based on the role and access permissions.

    How this test will be performed: A test group of users will be given the system, and a set of non-admin user credentials. They will be asked to attempt various administrative operations using these credentials and it will be verified that the users were not able to access those options based on the permissions.

\end{enumerate}

\subsubsection{IR1}

\begin{enumerate}

    \item{test-IR1-1}  \label{test-IR1-1}
    
    Type: Non-functional, Dynamic, Automated
    
    Initial State: System is deployed and accessible.
    
    Input: Simulate multiple concurrent failed login attempts while monitoring credential storage.

    Output: User credentials remain unchanged, and system maintains stability.
    
    How this test will be performed: A test group of users and the supervisor will be given the system, and a set of test credentials for concurrent authentication testing. They will be asked to perform multiple simultaneous authentication attempts and it will be verified that all credentials remained intact and system stability was maintained.

  \end{enumerate}

\subsubsection{IR2}
  
  \begin{enumerate}

    \item{test-IR2-1} \label{test-IR2-1} 
    
    Type: Functional, Dynamic, Manual
    
    Initial State: The system is set up, open to the relevant section and ready to take the user input.
    
    Input: Submit an invalid data input.
    
    Output: An error message is displayed to the user with feedback to recover from error state. No data is added to any database. 
    
    How test will be performed: The test controller will input an invalid data input and check if the system is able to validate and reject the invalid data. The controller will verify that an error message appears.

  \end{enumerate}

\subsubsection{IR3}
    
  \begin{enumerate}


    \item{test-IR3-1}  \label{test-IR3-1}
    
    Type: Functional, Dynamic, Automated
    
    Initial State: The prediction model is ready for to predict trestment plan and diagnosis.
    
    Input: A test set of various patient medical charts.

    Output: The prediction includes a diagnosis and treatment prediction, each with a confidence score exceeding 85\%.
    
    How test will be performed: Automated testing of model outputs and confidence scores this assists in understanding how sure the model is in its classification.

  \end{enumerate}

  \subsubsection{IR4}
    
  \begin{enumerate}

    \item{test-IR4-1} \label{test-IR4-1}
    
    Type: Functional, Dynamic, Manual
    
    Initial State: System is set up, open to the relevant section, and ready to take user input. A document already exists in the relevant database.

    Input: A new input with the same data as an existing document.
  
    Output: An error message is displayed. No new document is added to the database.  
    
    How test will be performed: The test controller will have a new input with the same data as an existing document and check if the system is able to validate and reject the duplicate record. The controller will verify that an error message appears.

  \end{enumerate}

  \subsubsection{IR5}
    
  \begin{enumerate}

    \item{test-IR5-1} \label{test-IR5-1}   
    
    Type: Functional, Dynamic, Manual
    
    Initial State: System is set up, open to the relevant section, and ready to take user input.

    Input: Audio conversation between the patient and healthcare professional.

    Output: The transcribed text from the input data matches the conversation with minimal to no interference.

    How test will be performed: The test controller will have a new data input and check if the system is able to validate and classify the data accurately. The controller will verify that the generated text matches the spoken conversation with 90\% percent accuracy. The accuracy will be assessed by comparing the system's output to the human transcription and calculating the percentage of correct words.

\end{enumerate}

\newpage


\begin{landscape}
\subsection{Traceability Between Test Cases and Requirements} \label{section:4.4}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
   Test ID & FR1 & FR2 & FR3 & FR4 & FR5 & FR6 & FR7 & FR8 & FR9 & FR10 & FR11 & FR12 & FR13\\
  \hline
  test-FR1,4,8-\ref{test-FR1,4,8-1} & $\times$ & & & $\times$ & & & & $\times$ & & & & & \\
  \hline
  test-FR1,4,8-\ref{test-FR1,4,8-2} & $\times$ & & & $\times$ & & & & $\times$ & & & & & \\
  \hline
  test-FR2,5,9-\ref{test-FR2,5,9-1} & & $\times$ & & & $\times$ & & & & $\times$ & & & & \\
  \hline
  test-FR2,5,9-\ref{test-FR2,5,9-2} & & $\times$ & & & $\times$ & & & & $\times$ & & & & \\
  \hline
  test-FR3,6,10,11-\ref{test-FR3,6,10,11-1} & & & $\times$ & & & $\times$ & & & & $\times$ & $\times$ & & \\
  \hline
  test-FR3,6,10,11-\ref{test-FR3,6,10,11-2} & & & $\times$ & & & $\times$ & & & & $\times$ & $\times$ & & \\
  \hline
  test-FR7-\ref{test-FR7-1} & & & & & & & $\times$ & & & & & & \\
  \hline
  test-FR7-\ref{test-FR7-2} & & & & & & & $\times$ & & & & & & \\
  \hline
  test-FR11-\ref{test-FR11-1} & & & & & & & & & & & $\times$ & & \\
  \hline
  test-FR11-\ref{test-FR11-2} & & & & & & & & & & & $\times$ & & \\
  \hline
  test-FR12,13-\ref{test-FR12,13-1} & & & & & & & & & & & & $\times$ & $\times$ \\
  \hline
  test-FR12,13-\ref{test-FR12,13-2} & & & & & & & & & & & & $\times$ & $\times$ \\
  \hline
  test-FR12,13-\ref{test-FR12,13-3} & & & & & & & & & & & & $\times$ & $\times$ \\
  \hline
\end{tabular}
\caption{\bf Functional Requirements Tests Traceability} \label{tab:fr-test-traceability}
\end{table}


\begin{table} [H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    TestID & NFR1 & NFR2 & NFR3 & NFR4 & NFR5 & NFR6 & NFR7 & NFR8 & NFR9 \\
    \hline
    test-AD\ref{test-AD1} & $\times$ & & & & & & & & \\
    \hline
    test-AD\ref{test-AD2} & $\times$ & & & & & & & & \\
    \hline
    test-UR\ref{test-UR1} & & $\times$ & & & & & & &  \\
    \hline
    test-PR\ref{test-PR1} & & & $\times$ & & & & & & \\
    \hline
    test-OR\ref{test-OR1} & & & & $\times$ & & & & &  \\
    \hline
    test-MR\ref{test-MR1} & & & & & $\times$ & & & &\\
    \hline
    test-SR\ref{test-SR1} & & & & & & $\times$ & & & \\
    \hline
    test-CR\ref{test-CR1} & & & & & & & $\times$ & &\\
    \hline
    test-LR\ref{test-LR1} & & & & & & & & $\times$ &\\
    \hline
    test-S\ref{test-S1} & & & & & & & & & $\times$ \\
    \hline
  \end{tabular}
\caption{\bf Non-Functional Requirements Tests Traceability} \label{tab:nfr-test-traceability}
\end{table}

\begin{table} [H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
  Test ID & AC1 & AC2 & IR1 & IR2 & IR3 & IR4 & IR5 \\
  \hline
  test-AC1-\ref{test-AC1-1} & $\times$ & & & & & & \\
  \hline
  test-AC2-\ref{test-AC2-1} & & $\times$ & & & & &  \\
  \hline
  test-IR1-\ref{test-IR1-1} & & & $\times$ & & & &  \\
  \hline
  test-IR2-\ref{test-IR2-1}  & & & & $\times$ & & & \\
  \hline
  test-IR3-\ref{test-IR3-1}  & & & & & $\times$ & & \\
  \hline
  test-IR4-\ref{test-IR4-1}  & & & & & & $\times$ & \\
  \hline
  test-IR5-\ref{test-IR5-1}  & & & & & & & $\times$  \\
  \hline
\end{tabular}
\caption{\bf Safety and Security Requirements Tests Traceability} \label{tab:sns-test-traceability}
\end{table}

\end{landscape}
\newpage


\section {Unit Test Description}

\subsection{Unit Testing Scope}

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

\section{Unit Testing Scope}

 While comprehensive testing will e performed on several critical modules, certain modules like Serivce Layer module, Patient and Administrator model modules, and views in Patient View and Adminstrator view modules will not tested. This is due to various reasons.\\
 Service Layer Module was not included in the unit testing scope as it used primarily for routing interactions and focus was placed on testing the core functionalities rather than the service layer, which is expected to be stable and well-defined.\\
 Patient Model and Administrator Model Modules were not included in unit testing scope as they just provide the data structure for for various objects. The tests for integrity of the data are included in the Data Layer Module tests.\\
UI Components in Administrator View and Patient View Modules were not included in unit testing. This is because user acceptance testing and usability testing would provide better insights for overall user experience and interaction with the system can be evaluated in a more holistic manner.\\
By prioritizing the verification of core functionalities, we aimed to ensure that the most essential aspects of the system were thoroughly tested.

\subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

This section will include unit tests for functional requirements organised by different modules Details of the functional requiremnts can be found in \href{https://github.com/Inreet-Kaur/capstone/blob/main/docs/SRS/SRS.pdf}{SRS} and details about different modules can be found in \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS} and \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}.

\subsubsection{User Authentication Module}

This subsection covers User Authentication Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. This module focuses on the authentication mechanism, including secure storage and validation of user credentials, and session management. The tests outlined below are designed to verify both the functionality and security of the authentication mechanisms. These tests were selected based on the situations users will encounter under normal conditions and also considers potential edge cases to ensure comprehensive coverage of the authentication functionality.

\textbf{Control:} Automatic \\ 
\textbf{Initial State:} The default email and password are initialized as variables and given a default value.\\ 
\textbf{Test Case Derivation:} The expected behavior is derived from the correct registration and login functionality of Firebase Authentication, ensuring valid user creation and authentication processes, as well as error handling for invalid credentials.\\ 
\textbf{Test Procedure:} The tests are performed as follows:
\begin{enumerate}
  \item Sign up using valid credentials:
    \begin{itemize}
      \item \textbf{Input:} The input for this test are the valid credentials of email and password.
      \item \textbf{Output:} The user object is returned by the signup function.
      \item \textbf{Test Derivation:} Verifies that the signup function correctly works and creates a new user account when the valid credentials are provided.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Sign up throwing errors with invalid credentials:
    \begin{itemize}
      \item \textbf{Input:} The input for this test are the invalid credentials of email and password.
      \item \textbf{Output:} An error thrown up by the signup function.
      \item \textbf{Test Derivation:} Verifies that the signup function correctly throws error when the invalid credentials are provided.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Login using valid credentials:
    \begin{itemize}
      \item \textbf{Input:} The input for this test are the valid credentials of email and password. 
      \item \textbf{Output:} The user object is returned by the signin function.
      \item \textbf{Test Derivation:} Verifies that the signin function correctly authenticates the user when valid credentials are provided.
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Login throwing errors with invalid credentials:
    \begin{itemize}
      \item \textbf{Input:} The input for this test are the invalid credentials of email and password. 
      \item \textbf{Output:} An error thrown up by the signin function.
      \item \textbf{Test Derivation:} Verifies that the signin function correctly throws error when the invalid credentials are provided.
      \item \textbf{How test will be performed:} 
  \end{itemize}
\end{enumerate}

\subsubsection{Data Layer Module}

This subsection covers Data Layer Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. The Data Layer Module is responsible for managing the storage and retrieval of data, including patient records and healthcare professionals records. The tests in this section were selected to validate the core functionalities of adding, updating, deleting, and retrieving data. They encompass both typical user interactions and potential error conditions to ensure that the data layer operates reliably and securely.

\textbf{Control:} Automatic\\ 
\textbf{Initial State:} Provides mock data of healthcare professionals, healthcare networks and patients.\\ 
 \textbf{Test Case Derivation:} The expected behavior is derived from the correct functionality of onboarding, updating, and removing the healthcare networks, healthcare professionals and patient records.\\ 
\textbf{Test Procedure:} The tests are performed as follows:
\begin{enumerate}
  \item Adding valid documents to the database:
    \begin{itemize}
      \item \textbf{Input:} Mock data of the healthcare professionals, healthcare networks and patients are given as input.  
      \item \textbf{Output:} Successful addition of valid information of healthcare networks, healthcare professionals and patients in the database. 
      \item \textbf{Test Derivation:} Verifies that addHealthcareProfessional, addHospital and addPatient functions work correctly ensuring proper document creation. The functions addHealthcareProfessional and addHospital add the documents if they don't exist in the database. However, if they do, then these functions update the data in those documents. The function addPatient is only used to add a new patient record in the database.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item No duplicate addition of patient document in the system:
    \begin{itemize}
      \item \textbf{Input:} Mock data of the patients are given as input.  
      \item \textbf{Output:} No specific output is returned by the test function itself. However, the test asserts a specific state in the database. 
      \item \textbf{Test Derivation:} Verifies that the addPatient function correctly handles attempts to add duplicate patient records. It ensures that only one patient record is created even if the addPatient function is called multiple times.
      \item \textbf{How test will be performed:}
    \end{itemize}

    Note: Test for invalid document format: \\ 
    Since, we have implemented validate input function in the UI module, this ensures that no invalid output is taken by the system. In addition to this, TypeScript has type checking on the parameters therefore invalid objects cannot be passed.

  \item Deleting existing documents from the database:
    \begin{itemize}
      \item \textbf{Input:} Mock data of the healthcare professionals, healthcare networks and patients are given as input.  
      \item \textbf{Output:} Successful deletion of valid information of healthcare networks, healthcare professionals and patients in the database. 
      \item \textbf{Test Derivation:} Verifies that deleteHealthCareProfessional, deleteHospital and deletePatient functions work correctly ensuring proper document deletion.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Updating existing patient documents in the database:
    \begin{itemize}
      \item \textbf{Input:} Mock data of the patients are given as input.  
      \item \textbf{Output:} Successful update of valid information of the patients in the database. 
      \item \textbf{Test Derivation:} Verifies that updatePatient function works correctly ensuring proper patient record update.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Retrieve correct existing document:
    \begin{itemize}
      \item \textbf{Input:} Mock id of the patients are given as input.  
      \item \textbf{Output:} Retrieval of the correct patient object from the database. 
      \item \textbf{Test Derivation:} Verifies that the getPatient function correctly retrieves an existing patient document from the database after providing patient id.
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Retrieve non-existing document:
    \begin{itemize}
      \item \textbf{Input:} Non-existent mock id of the patients are given as input.  
      \item \textbf{Output:} Returns null. 
      \item \textbf{Test Derivation:} Verifies that the getPatient function correctly handles situations when a non-existing patient document from the database is requested to retrieve after providing a non-existing patient id.
      \item \textbf{How test will be performed:} 
    \end{itemize}
\end{enumerate}

\subsubsection{Administrator View Module and Patient View Module}

This subsection covers Administrator View Module and Patient View Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. These modules provide the user interface for both administrators and patients. The tests included in this section were used to verify correct behaviour of different functions implemented to verify input in different fields ensuring that the system correctly rejects an invalid input by the user and cover all edge cases.

\textbf{Control:} Automatic\\ 
\textbf{Initial State:} No specific initialization is required as tests solely focus on validating fields.\\ 
\textbf{Test Case Derivation:} The expected behavior is derived from the correct implementation of data validation rules and age calculation logic, ensuring accurate identification of invalid input, acceptance of valid input, and precise calculation of age based on provided dates.\\ 
\textbf{Test Procedure:} The tests are performed as follows:\\
\begin{enumerate}
  \item Should return appropriate error for invalid field input:
    \begin{itemize}
      \item \textbf{Input:} The inputs for this test are the actual value of the field and a string representing the field type.   
      \item \textbf{Output:} Returns appropriate error messages on receiving invalid input.
      \item \textbf{Test Derivation:} Verifies that the validateField function correctly identifies invalid input values for different field types. It ensures that the function returns appropriate error messages for invalid email formats, phone numbers, dates, and numerical values for age, weight, and height.
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Should return an empty string for valid field input:
    \begin{itemize}
      \item \textbf{Input:} The inputs for this test are the actual value of the field and a string representing the field type.   
      \item \textbf{Output:} Returns an empty string on receiving valid input.
      \item \textbf{Test Derivation:} Verifies that the validateField function correctly identifies valid input values for different field types. It ensures that the function returns an empty string for valid email formats, phone numbers, dates, and numerical values for age, weight, and height.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Correct calculation of age:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the date of birth.    
      \item \textbf{Output:} Returns a number representing the calculated age.
      \item \textbf{Test Derivation:} Verifies that the calculateAge function correctly calculates the age for a given date of birth.
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Calculation of age for future dates:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the date of birth in the future.    
      \item \textbf{Output:} Returns 0.
      \item \textbf{Test Derivation:} Verifies that the calculateAge function correctly handles the age for a given date of birth in the future.
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Calculation of age for the present date:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the date of birth representing today's date.    
      \item \textbf{Output:} Returns 0.
      \item \textbf{Test Derivation:} Verifies that the calculateAge function correctly handles the age for a given date of birth with today's date.
      \item \textbf{How test will be performed:} 
    \end{itemize}
\end{enumerate}

\subsubsection{Diagnosis and Treatment Plan Prediction Module}

This subsection covers Diagnosis and Treatment Plan Prediction Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. This module is responsible for providing diagnosis and treatment plan based on the transcribed text from the patient doctor interaction. The test below were selected to verify the correctness and accuracy of the prediction based on the given context. However, these test are just preliminary analysis further stress testing would be required to verify the accuracy of the predictions.

\textbf{Control:} Automatic\\ 
\textbf{Initial State:} A predefined set of conversation is used for testing. It also assumes that Open API Key is configured correctly.\\ 
\textbf{Test Case Derivation:} The expected behavior is derived from the correct functioning of an API endpoint that processes patient inputs and provides relevant responses, ensuring accurate handling of valid and invalid input data, as well as a high confidence level in generating appropriate responses. We will review the manual test by our supervisor.\\ 
\textbf{Test Procedure:} The tests are performed as follows:\\

\begin{enumerate}
  \item Confidence score test:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the list of strings which represent patient conversation.   
      \item \textbf{Output:} There is no direct output. This test asserts a condition based on the number of successful evaluations.
      \item \textbf{Test Derivation:} Verifies that the testConfidence function correctly sends each conversation to the API endpoint. The test calculates the success rate based on the number of relevant responses and asserts that the success rate is above a certain threshold (0.85). Due to the output of this test not being the deterministic, we have used a strategy that involves using a third independent AI agent to evaluate the confidence. This allows for an initial benchmark of the confidence on the service's performance. 
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Valid data input test:
    \begin{itemize}
      \item \textbf{Input:} A dictionary that contains a valid transcription of a patient conversation. 
      \item \textbf{Output:} Returns a response object from API endpoint.
      \item \textbf{Test Derivation:} Verifies that the API endpoint correctly handles and processes valid input data. It sends POST request with valid transcription to the API endpoint and asserts that the response status code is 200, indicating successful processing of the input.
      \item \textbf{How test will be performed:} 
    \end{itemize}

  \item Invalid data input test:
    \begin{itemize}
      \item \textbf{Input:} A dictionary that contains an empty string of invalid transcription of a patient conversation.    
      \item \textbf{Output:} Returns a response object from API endpoint.
      \item \textbf{Test Derivation:} Verifies that the API endpoint correctly handles and processes invalid input data. It sends POST request with invalid and missing transcription to the API endpoint and asserts that the response status code is 400, successful identification of invalid input.
      \item \textbf{How test will be performed:} 
    \end{itemize}
\end{enumerate}

\subsubsection{Transcription Module}

This subsection covers Transcription Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. The Transcription Module is responsible for converting audio input from patient interactions into text format. The tests in this section were selected to verify the accuracy and reliability of the transcription process. The tests include both normal and edge scanerios to ensure that the system can handle a range of valid and invalid inputs effectively and meet the critical need for accurate transcriptions in healthcare settings.

\textbf{Control:} Manual\\
\textbf{Initial State:} The audio files are provided as input for the tests and Socket.IO Server is running at a specified host.\\
\textbf{Test Case Derivation:} The expected behavior is derived from the correct functioning of a Socket.IO server that receives audio chunks, performs transcription, and emits transcription results or errors. The tests ensure that the server correctly transcribes valid audio data to match the asserted string, handles invalid audio input, and emits appropriate events with accurate transcription results or error indicators.\\
\textbf{Test Procedure:} The tests are performed as follows:\\

\begin{enumerate}
  \item Test for valid audio file:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the path to the valid audio file.   
      \item \textbf{Output:} Asserts the transcribed text equal to the string containing expected output.
      \item \textbf{Test Derivation:} Verifies the audio transcription functionality of the Socket.IO server. It establishes a connection with the server and sends the audio data in chunks and then emits the transcribed text. 
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Test for invalid audio file:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the path to the invalid audio file. 
      \item \textbf{Output:} Returns an empty string.  
      \item \textbf{Test Derivation:} Verifies that the API endpoint correctly handles invalid input data. It establishes the connection with the server and asserts that transcription\_text remains empty indicating that the invalid input is correctly handled.
      \item \textbf{How test will be performed:}
    \end{itemize}
\end{enumerate}

\subsubsection{Classification Module}

This subsection covers Classification Module from \href{https://github.com/PKALXI/RapidCare/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}. The Classification Module processes the transcribed text to automatically fill patients charts with appropriate values for each fields such as symptoms, allergies, and medications. The tests included in this section were chosen to assess whether the service can accurately classify and extract relevant data ensuring that it meets the necessary performance and accuracy standards.

\textbf{Control:} Automatic\\
\textbf{Initial State:} A list of pre-defined conversation containing patient's symptoms, allergies, medications is initialized.\\
\textbf{Test Case Derivation:} The expected behavior is derived from the correct functioning of an API endpoint that processes patient conversations, extracts key information (symptoms, reason for visit, allergies, current medications), and provides relevant responses. The tests ensure that the API accurately extracts information from various conversation formats, handles both valid and invalid input data, and maintains a high level of confidence in generating appropriate responses.\\
\textbf{Test Procedure:} The tests are performed as follows:\\

\begin{enumerate}
  \item Confidence score test:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the list of strings representing the patient conversation consisting of medications, allergies and current medications.   
      \item \textbf{Output:} There is no direct output. This test asserts a condition based on the number of successful evaluations.
      \item \textbf{Test Derivation:} Verifies the confidence of the API in correctly extracting information from patient conversations. It iterates through the list sending conversation to the API endpoint. We will test it manually with our supervisor. Due to the output of this test not being the deterministic, we have used a strategy that involves using a third independent AI agent to evaluate the confidence. This allows for an initial benchmark of the confidence on the service's performance.  
      \item \textbf{How test will be performed:}
    \end{itemize}

  \item Test for valid input:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is the valid patient transcription. 
      \item \textbf{Output:} Returns a response object from the API endpoint. 
      \item \textbf{Test Derivation:} Verifies that the API endpoint correctly handles and processes valid input data. It sends POST request with valid input to the API endpoint and asserts that the response status code is 200, indicating successful processing of the input.
      \item \textbf{How test will be performed:}
    \end{itemize}

    \item Test for invalid input:
    \begin{itemize}
      \item \textbf{Input:} The input for this test is an empty dictionary. 
      \item \textbf{Output:} Returns a response object from the API endpoint. 
      \item \textbf{Test Derivation:} Verifies that the API endpoint correctly handles empty input data. It sends POST request with empty input to the API endpoint and asserts that the response status code is 400, indicating successful processing of the input.
      \item \textbf{How test will be performed:}
    \end{itemize}
\end{enumerate}


\newpage
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}



\newpage

\section{Appendix} \label{section:6}

\subsection{Symbolic Parameters}  \label{section:6.1}

N/A

\subsection{Usability Survey Questions?} \label{section:6.2}

% \wss{This is a section that would be appropriate for some projects.}

The following multiple choice questions will be presented to the user who will be using this software.
\begin{enumerate}
  
\item What operating system do you use?
  \begin{enumerate}
    \item Mac
    \item Windows
  \end{enumerate}

\item Is the system compatible with the hardware?
  \begin{enumerate}
    \item Yes
    \item No
  \end{enumerate}

\item Is the output accurate when generated with sample inputs?
  \begin{enumerate}
    \item Yes
    \item No
  \end{enumerate}

\item Does this system satisfy all the needs of the user?
  \begin{enumerate}
    \item Yes
    \item No
  \end{enumerate}

\item How intuitive the software is? How easy is it to find functions? Rate performance.
  \begin{enumerate}
    \item Very intuitive
    \item Intuitive
    \item Neutral
    \item Not intuitive
    \item Very confusing
  \end{enumerate}

\item Are there any other functionalities that you want to list which can further improve the performance of this software?

\end{enumerate}


\newpage{}
\section*{Appendix --- Reflection}

% \wss{This section is not required for CAS 741}
The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable?\\
  This document has let us build more on the rough ideas of the requirements that we had brainstormed initially. While going through the outline of this document, we were able to provide the tests for functional requirements, non-functional requirements, and safety and security requirements. It also made us better understand the detailed procedure of system testing and plan for design validation and verification. 
  
  \item What pain points did you experience during this deliverable, and how did you resolve them?\\
  There are obstacles in any team project that must be overcome for it to proceed successfully. To ensure seamless operations, we had to develop a strategy for contributions. We had to collectively make a list of tests that are needed to be conducted for functional requirements, non-functional requirements, and safety and security requirements. We also needed to create a schedule to contribute to the template and review each other's work in the best way possible.

  \item What knowledge and skills will the team collectively need to acquire to successfully complete the verification and validation of your project? Examples of possible knowledge and skills include dynamic testing knowledge, static testing knowledge, specific tool usage, Valgrind etc.  You should look to identify at least one item for each team member.\\
  As different abilities are added to the overall development plan, the project progresses more quickly thanks to the diversified knowledge of the team members. Technical expertise such as static and dynamic testing knowledge are very helpful to come up with a successful verification and validation plan for our project. Knowledge about how the external libraries can be used is also an asset for validating the system. Finally, being able to work on backend programming with an understanding of different server-side technologies, like Python, Java, and React.js will be helpful in creating a dynamic database that secures patient data.

  \item For each of the knowledge areas and skills identified in the previous question, what are at least two approaches to acquiring the knowledge or mastering the skill?  Of the identified approaches, which will each team member pursue, and why did they make this choice?\\
  One can use a variety of resources to learn Java from Spring and Python from Flask to become effective in backend programming. With these resources, developers can gain practical experience by building web applications that offer them flexibility. Using LeetCode to practice coding skills is an additional strategy. This allows users to modify the difficulty of the problems and proceed with their solution. Moreover, setting goals and taking regular pauses between tasks might help with time management and allow one to work more effectively. It's critical that each member of the team grasp every talent for everyone to be moving at the same speed. This is because it's a fantastic chance to learn and implement it in practical situations.
\end{enumerate}

\end{document}